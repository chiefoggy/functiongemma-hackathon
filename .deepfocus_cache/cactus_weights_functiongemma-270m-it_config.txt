vocab_size=262144
hidden_dim=640
num_layers=18
attention_heads=4
attention_kv_heads=1
ffn_intermediate_dim=2048
context_length=32768
rope_theta=10000.0
attention_head_dim=256
layer_norm_eps=1e-06
num_experts=0
num_shared_experts=0
num_top_experts=0
moe_every_n_layers=0
tie_word_embeddings=true
model_type=gemma
model_variant=default
precision=FP16
