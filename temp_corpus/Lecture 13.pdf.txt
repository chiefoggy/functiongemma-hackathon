coordinates relative to S
Dimension
Theorem 3.5.11
Theorem 3.6.1
How to find the dimension of the solution space of HSL.
Lecture 13Dimension and Transition Matrices

Theorem 3.6.7Let  Vbe a vector space of dimension  kand  Sa subset of  V.The following are equivalent:1.Sis a basis for  V.2.Sis linearly independent and  |S| = k.3.Sspans  Vand  |S| = k.
 1⇔2⇔3Goal:It is enough to show that             and                     Recall  Sis a basisfor  VifSis linearly independentand SspansV.Remark 3.6.2: All bases for a vector space have the same numberof vectors, i.e., the cardinality of a basis.“1   2”and  “13”follows from Remark 3.6.2.⇒⇒We need to show that           and  
 2⇔1 3⇔1 2⇒1 3⇒1
Proof of Theorem 3.6.7“1   2”and  “13”follows from Remark 3.6.2.2  1:Suppose  Sis linearly independent and  |S| = k.Assumethat  Sis not a basis for  V, i.e.  span(S) V.⇒⇒⇒≠By Theorem 3.4.10,  S’= S {u}is a set of  k+ 1linearly independent vectors.But this contradictsTheorem 3.6.1.∪Theorem 3.4.10: Suppose  u1,u2,…,ukare linearly independent vectors in  Rn. If  uk+1is not a linear combination of  u1,u2,…,uk,  then  u1,u2,…,uk, uk+1are linearly independent.
Take a vector  uin  Vbut not in  span(S).
Proof of Theorem 3.6.73 1:Suppose  Sspans  Vand  |S| = k.Assumethat  Sis not a basis for  V,i.e.  Sis linearly dependent.Take a vector  vin  Swhich is a linear combination of other vectors in  S.
⇒By Theorem 3.2.12,  S”= S–{v}is a set of  k–1vectors that spans  V.But this also contradictsTheorem 3.6.1.
Theorem 3.2.12: Suppose  u1,u2,…,ukare vectors taken from  Rn. Show that if  ukis a linear combination of  u1,u2,…,uk–1,  then span{u1, u2, …, uk–1} = span{u1, u2, …, uk}.
Example 3.6.8  QuestionShow thatu1= (5, –4, 2),  u2= (2, –1, –3)and  u3= (3, 1, –1)form a basis for  R3.
Example 3.6.8 Solution0321=++uuu321ccc0,0,0321===ÞcccSo  u1, u2, u3are linearly independent.Since  dim(R3) = 3,  by Theorem 3.6.7,  {u1, u2, u3}is a basis for  R3.
  ⇒c1(5,−4,2)+c2(2,−1,−3)+c3(3,1,−1)=(0,0,0)  ⇒5c1+2c2+3c3=0−4c1−c2+c3=02c1−3c2−c3=0⎧⎨⎪⎪⎩⎪⎪
Theorem 3.6.9Let  Ube a vector space and  Ua subspace of  V.Then  dim(U)   dim(V).If  UV,  then  dim(U) < dim(V).≤≠
Proof of Theorem 3.6.9Let Sbe a basis for U. Since UV, Sis a linearly independent subset of V. By Theorem 3.6.1.1, dim(U)=|S|    dim(V). Assume dim(U) = dim(V). As Sis linearly independent and |S| = dim(U)=dim(V), by Theorem 3.6.7, Sis a basis for V. But then U= span(S)=V. Hence if U   V, then dim(U) < dim(V). 
⊆≤≠
Example 3.6.10Let Vbe a plane in Rncontaining the origin. Note that Vis a vector space of dimension 2, see Example 3.6.4.3. Suppose Uis a subspace of Vsuch that U   V. By Theorem 3.6.9, dim(U) < 2. Hence Uis either {(0,0,0)}(which is of dimension 0) or a line through the origin (which is of dimension 1). 
≠the originUV the originUV
Theorem 3.6.11Let  Abe a square matrix of order  n.The following statements are equivalent:1.Ais invertible.2.The linear system Ax = 0has only the trivial solution. 3.The reduced row-echelon form of Ais an identity matrix. 4.Acan be expressed as a product of elementary matrices. 5.det(A) is nonzero.6.The rows of  Aform a basis for  Rn.7.The columns of  Aform a basis for  Rn.
Proof of Theorem 3.6.11By Theorem 2.4.7, statements 1 to 4 are equivalent. By Theorem 2.5.19, we have “1    5”. The rows of  Aare the columns of  AT.Since  Ais invertible if and only if  ATis invertible (see Theorem 2.3.9), “6     7”.
⇔So, we only need to show either  “1 6”or “17”.
⇔⇔⇔
Proof of Theorem 3.6.11To show “17”:Let  A= (a1a2···  an)where aiis the ith column of  A. {a1, a2, …, an}is a basis for  Rn.a1,a2,…,anare linearly independent  (see Theorem 3.6.7)the equation  c1a1+ c2a2+ ···+ cnan= 0has only the trivial solution  (see Discussion 3.2.5)the system  Ac= 0has only the trivial solution where  c= (c1, c2, …, cn)TAis invertible  (see Theorem 2.4.10)
⇔⇔⇔⇔⇔
Example 3.6.12.1u1= (1, 2, –5),  u2= (0, 5, 4),  u3= (1, 2, 1)
{u1,  u2,  u3}is a basis for  R3. 101252−541=30
Example 3.6.12.2u1= (1, 2, –2, 3),  u2= (1, 3, 1, –3),  u3= (0, 4, 2,–1),  u4= (2, 1, –3, 1)
{u1,  u2,  u3 ,  u4}is not a basis for  R4. 12−23131−3042−121−31=0
Notation 3.7.1Let  S= {u1, u2, …, uk}be a basis for a vector space  V  and let  vbe a vector in  V.Recall thatif  v= c1u1+ c2u2+ ···+ ckuk,  then the vector  (v)S= (c1, c2, …, ck)is called thecoordinate vectorof  vrelative to  S.Sometimes, it is more convenient to write the coordinate vector in the form of a column vector.
÷÷÷÷÷øöçççççèæ=kScccv!21][Thus we define
Discussion 3.7.2Let  S= {u1, u2, …, uk}and  T= {v1, v2, …, vk}be two bases for a vector space  V.For any vector  win V,  we want to study the relation between  [w]Sand  [w]T.Let  w= c1u1+ c2u2+ ···+ ckukand
ïïîïïíì+++=+++=+++=kkkkvvvuvvvuvvvukkkkkkaaaaaaaaa!"!!21212211212221212111
Discussion 3.7.2i.e.
÷÷÷÷÷øöçççççèæ=÷÷÷÷÷øöçççççèæ=÷÷÷÷÷øöçççççèæ=÷÷÷÷÷øöçççççèæ=kkkkTkTkTkSaaaaaaaaaccc!"!!!21222121211121][][][][kuuuw21w= c1u1+ c2u2+ ··· + ckuk
= (c1a11+ c2a12+ ··· + cka1k)v1+ (c1a21+ c2a22+ ··· + cka2k)v2 + ··· + (c1ak1+ c2ak2+ ··· + ckakk)vk
= c1(a11v1+ a21v2+ ··· + ak1vk)+ c2(a12v1+ a22v2+ ··· + ak2vk)+ ··· + ck(a1kv1+ a2kv2+ ··· + akkvk)
Discussion 3.7.2i.e.
÷÷÷÷÷øöçççççèæ+++++++++=kkkkkkkkkTacacacacacacacacac!"!!221122222111122111][w÷÷÷÷÷øöçççççèæ÷÷÷÷÷øöçççççèæ=kkkkkkkcccaaaaaaaaa!"!!!""21212222111211()STTT][][][][wuuuk!21=The matrix  P= ( [u1]T[u2]T··· [uk]T)is called the transition matrixfrom  Sto  T.[w]T= P[w]Sfor all  win  V.
Definition 3.7.3
The square matrix  P= ( [u1]T[u2]T··· [uk]T)is called the transition matrixfrom  Sto  T.[w]T= P[w]Sfor all  win  V.
Let  S= {u1, u2, …, uk}and  Tbe two bases for a vector space  V.
Example 3.7.4.1  QuestionLet  S= {u1, u2, u3}whereu1= (2, 3, 2),  u2= (2, 0, –3),  u3= (1, 3, 2).Let  T= {v1, v2, v3}wherev1= (1, 1, –1),  v2= (0, 1, 0),  v3= (1, 1, 0).Sand  Tare two bases for  R3.(a)Find the transition matrix from  Sto  T.(b)Let  wbe a vector in  R3with  (w)S= (2, –1, 2).  Find  (w)T.
Example 3.7.4.1(a)  SolutionFirst, we need to find  a11, a21, …, a33such thatu1= a11v1+ a21v2+ a31v3,u2= a12v1+ a22v2+ a32v3,u3= a13v1+ a23v2+ a33v3.
This is equivalent to solve the following three linear systems:
  a11+a31=2a11+a21+a31=3−a11=2⎧⎨⎪⎪⎩⎪⎪a11+a31=2a11+a21+a31=0−a11=−3⎧⎨⎪⎪⎩⎪⎪a11+a31=1a11+a21+a31=3−a11=2⎧⎨⎪⎪⎩⎪⎪S T
Example 3.7.4.1(a)  SolutionGauss-JordanElimination
We have  u1= –2v1+  v2+ 4v3u2= 3v1–2v2–v3u3= –2v1+ 2v2+ 3v3So the transition matrix from  Sto  Tis
 101221111303−1002−32⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟ ⎯→⎯100−23−20101−220014−13⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟  P=−23−21−224−13⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟
Example 3.7.4.1(b)
So  (w)T= (–11, 8, 15).   [w]T=P[w]S=−23−21−224−13⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟2−12⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟=−11815⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟
Example 3.7.4.2Let  S= {u1, u2}where  u1= (1, –1),  u2= (–1, 2).Let  T= {v1, v2}where  v1= (1, 1),  v2= (0, 1).Sand  Tare two bases for  R2.
îíì-==21221vvuvu2÷øöçèæ-=Þ1120Pis the transition matrix from  Sto  Tis the transition matrix from  Tto  S   v1=3u1+2u2v2=u1+u2⎧⎨⎪⎩⎪  ⇒Q=3121⎛⎝⎜⎞⎠⎟   u1=v1−2v2u2=−v1+3v2⎧⎨⎪⎩⎪
Remark on Example 3.7.4.2is the transition matrix from  Sto  T
is the transition matrix from  Tto  SNote that  Qis the inverse of  P.  Q=3121⎛⎝⎜⎞⎠⎟  P=1−1−23⎛⎝⎜⎞⎠⎟
Theorem 3.7.5Let  Sand  Tbe two bases of a vector space andlet  Pbe the transition matrix from  Sto  T.1.Pis invertible.2.P–1is the transition matrix from  Tto  S.
Proof of Theorem 3.7.5Let  P’be the transition matrix from  Tto  S.
By Theorem 2.4.12, it suffices to show  P’P= I.Suppose  S= {u1, u2, …, uk}.
÷÷÷÷÷÷øöççççççèæ=÷÷÷÷÷÷øöççççççèæ=÷÷÷÷÷÷øöççççççèæ=1000][0010][0001][!"!!SSSkuuu21Theorem 2.4.12: If  AB =I,  then A, Bare invertible, A–1= B,  B–1= A,  and  BA =I.
Proof of Theorem 3.7.5An observation:
÷÷÷÷÷÷÷÷÷øöçççççççççèæ÷÷÷÷÷÷øöççççççèæ--+-+-001001,1,121,221,22111,111,111!!""!!!!!!!!!""""mnimmiimmniiiniiiaaaaaaaaaaaaaaaith columnith coordinate
÷÷÷÷÷÷øöççççççèæ=miiiaaa!!21
Proof of Theorem 3.7.5For  i= 1, 2, …, k,( theith column of P’P )= P’P[ui]S
Note that  Pis the transition matrix from  Sto  T= P’[ui]TNote that  P’is the transition matrix from  Tto  SSo  P’P= ( [u1]S[u2]S···  [uk]S) = I.= [ui]S
Example 3.7.6In Example 3.7.4.1(a), the transition matrix from  Sto  Tis
So the transition matrix from  Tto  Sis  P=−23−21−224−13⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟  P−1=−23−21−224−13⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟−1=−49−79295929297910919⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟
Example 3.7.6In Example 3.7.4.1(b),  (w)S= (2, –1, 2)  and(w)T= (–11, 8, 15).
   P−1[w]T=−49−79295929297910919⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟−11815⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟=2−12⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟=[w]S
Learning outcomes (Section 3.6)(4) (Theorem 3.6.7) Suppose Vis a vector space with dimension k. Then any linearly independent subset of Vwith kvectors is a basis for V. Similarly, any subset of Vwith kvectors that spans Vis a basis for V. (5) All subspaces of a vector space Vhas dimension no larger than the dimension of V. Furthermore, the only subspace of Vwith the same dimension as Vis Vitself. (6) Two more equivalent statements to ‘Ais an invertible square matrix’in terms of basis for Rn. 
Learning outcomes (Section 3.7)(1) If Sand Tare two bases for the vector space V, for any vector vin V, is there a relationship between (v)Sand (v)T? 
(2) Definition of transition matrix from Sto T. 
(3) All transition matrices A(say from Sto T) are invertible and A–1is also a transition matrix (from Tto S). 