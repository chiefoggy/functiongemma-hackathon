Least Squares Fitting 

Learning outcomes (Section 5.2) 
(6) What is orthogonal projection of a vector u onto a 
vector space V?  
(7) (Theorem 5.2.15) If we have an orthogonal (resp. 
orthonormal) basis for a vector space V, how can we 
project a vector w onto V? Why is Theorem 5.2.8 a special 
case of Theorem 5.2.15?  
(8) (Theorem 5.2.19) What happens if we do not have an 
orthogonal basis for V? How can we convert a (non-
orthogonal) basis for V into an orthogonal basis? (Gram-
Schmidt Process).  
Learning outcomes (Section 5.3) 
(1) (Theorem 5.3.2) Let V be a subspace in Rn and u is a 
vector in Rn. Which vector in V is the best approximation 
of u (in V)? In other words, if you need to choose a vector 
in V that is the closest to u, which vector would you 
choose?  
(2) How we can use Theorem 5.3.2 to derive the method 
of least squares? The experimental data fitting example.  
(3) What is a least squares solution to a linear system Ax =  
b?  
(4) (Theorem 5.3.8) How we can use Theorem 5.3.2 to 
find a least squares solution to Ax = b? (the method in this 
theorem uses orthogonal projection onto the column space 
of A.)  
Lecture 18 
Orthonormal Bases (Continued) 
and 
Best Approximations 
Theorem 5.3.10  (Least Squares) 
Let Ax = b be a linear system.  
 
Then u is a least squares solution to Ax = b  
if and only if u is a solution to ATAx = ATb.  
Proof of Theorem 5.3.10 
Let A = (a1  a2  ···  an), where ai  is i-th  column of 
A, and let V be the column space of A, i.e., V = 
span{a1, a2, ..., an}= {Av | v ∈ Rn}. Then 
        u  is the least square solution 
        Au  is the projection of  b  onto V  (Theorem 5.3.8) 
       b – Au  is orthogonal to V  (Definition 5.2.13) 
⇔⇔  b – Au  is orthogonal to a1, a2, ..., an. (Remark 
5.2.12) 
⇔    ⇔a1⋅(b-Au)=0a2⋅(b-Au)=0!an⋅(b-Au)=0Because                             
(Dot product vs. the matrix multiplication), 
    aii(b−Au)=aiT(b−Au)
Proof of Theorem 5.3.10 
          ATb – ATAu = 0         
      ATAu = ATb 
⇔⇔  ⇔AT(b−Au)=0     ⇔a1T(b−Au)a2T(b−Au)!anT(b−Au)⎛⎝⎜⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟⎟=0    a1T(b−Au)a2T(b−Au)!anT(b−Au)⎛⎝⎜⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟⎟=a1Ta2T!anT⎛⎝⎜⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟⎟(b−Au)=AT(b−Au)because 
Example 5.3.11.1  (Least Squares) 
Use A and b in Example 5.3.9. In Example 5.3.9, find a 
least square solution to Ax = b. 
  A=22−554−6⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟b=7−34⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟  22−554−6⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟xy⎛⎝⎜⎞⎠⎟=6−52⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟⇔xy⎛⎝⎜⎞⎠⎟=21⎛⎝⎜⎞⎠⎟
Projection 	
Example 5.3.11.1  (Least Squares) 
Use A and b in Example 5.3.9. By Theorem 5.3.10, to find 
a least square solution to Ax = b, we do not need to 
compute the projection p.  
For this case, the equation ATAx = ATb is  
Solving this linear system, we obtain a least squares 
solution x = (2, 1)T.  
  2−5425−6⎛⎝⎜⎞⎠⎟22−554−6⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟xy⎛⎝⎜⎞⎠⎟=2−5425−6⎛⎝⎜⎞⎠⎟7−34⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟  45−45−4565⎛⎝⎜⎞⎠⎟xy⎛⎝⎜⎞⎠⎟=45−25⎛⎝⎜⎞⎠⎟
Example 5.3.11.2  (Least Squares) 
For our example, 
⎟⎟⎟⎠⎞⎜⎜⎜⎝⎛=⎟⎟⎟⎠⎞⎜⎜⎜⎝⎛⎟⎟⎟⎠⎞⎜⎜⎜⎝⎛⇒=7.161.246.47661061014101434edcTTbAAxAThe least square solution is  c = 0.9275,  d = 0.9225  
and  e = 0.3150. 
  A=001011121101411421⎛⎝⎜⎜⎜⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟⎟⎟⎟b=0.51.62.80.85.15.9⎛⎝⎜⎜⎜⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟⎟⎟⎟
Example 5.3.11.3  (Least Squares) 
In this example, we demonstrate how to find the 
projection using a least squares solution: 
 
Let V = span{(−1, 1, 1, 1), (−2, 2, 1, 1), (−1, 1, 2, 2)}.  
Find the projection of (4, 0, 0, 2)  onto V. 
  A=−1−2−1121112112⎛⎝⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟b=4002⎛⎝⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟Form a matrix: 
Solution of Example 5.3.11.3 
For this case, the equation ATAx = ATb is  
Solving this linear system, we obtain a least squares 
solution x = (4 – 3t, –3 + t, t)T, where t is an arbitrary 
parameter.  
  46661086810⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟xyz⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟=−2−60⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟Take any one of the least squares solutions, say  
x = (4, –3, 0)T and compute Au  
 −1−2−1121112112⎛⎝⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟4−30⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟=2−211⎛⎝⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟
Solution of Example 5.3.11.3 
  Au=−1−2−1121112112⎛⎝⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟4−30⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟=2−211⎛⎝⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟By Theorem 5.3.8, (2, –2, 1, 1) is the projection of (4, 0, 0,  
2) onto V.  
Discussion 5.4.1 
Since coordinate systems built upon orthonormal 
bases have a lot of advantages over coordinate 
systems using other bases, orthonormal bases are 
frequently used both in theoretic studies and in 
applications. 
On the other hand, in the studies of vector spaces, it 
is quite often that we need to shift between bases. 
 
Thus we would like to know more about transition 
matrices concerning orthonormal bases. 
Recall the Definition of Transition matrices  (Discussion 
3.7.2) 
Let  S = {u1, u2, …, uk}  and  T = {v1, v2, …, vk} be 
two bases for a vector space  V. 
The matrix  P = ( [u1]T  [u2]T ··· [uk]T )  is called the 
transition matrix from  S  to  T. 
 
For any vector  w  in  V,  [w]T = P [w]S. 
Example 5.4.2 
Let  S = {e1, e2 , e3}  be the standard basis for  R3, 
i.e. )1,0,0()0,1,0()0,0,1(===321eeeLet  T = {u1, u2 , u3} in where 
Both  S  and  T  are orthonormal bases for  R3. 
  u1=(−27,−37,67)u2=(37,−67,−27)u3=(−67,−27,−37)
Example 5.4.2 
The transition matrix from  T  to  S  is 
  u1=−27e1−37e2+67e3u2=37e1−67e2−27e3u3=−67e1−27e2−37e3⎧⎨⎪⎪⎩⎪⎪  P=−2737−67−37−67−2767−27−37⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟
Example 5.4.2 
The transition matrix from  S  to  T  is 
  e1=−27u1+37u2−67u3e2=−37u1−67u2−27u3e3=67u1−27u2−37u3⎧⎨⎪⎪⎩⎪⎪  Q=−27−376737−67−27−67−27−37⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟
Example 5.4.2 
Note that  Q = P T.  
By Theorem 3.7.5,  Q = P –1.  
So  P –1 = P T. 
  P=−27−376737−67−27−67−27−37⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟  Q=−2737−67−37−67−2767−27−37⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟
Definition 5.4.3 
A square matrix  A  is called orthogonal if  A –1 = A T. 
Remark 5.4.4 
By Theorem 2.4.12, a square matrix  A  is orthogonal 
if and only if  AAT = I  (or  ATA = I). 
Example 5.4.5 
The following are some examples of orthogonal 
matrices: 
 100010001⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟−27−376737−67−27−67−27−37⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟cos(θ)−sin(θ)sin(θ)cos(θ)⎛⎝⎜⎞⎠⎟
Theorem 5.4.6 
Let  A  be a square matrix of order  n. 
The following statements are equivalent:  
1.  A  is orthogonal. 
2. The rows of  A  form an orthonormal basis for  Rn. 
3. The columns of  A  form an orthonormal basis for  
Rn. 
For  i = 1, 2, …, n,  let  ai  be the ith row of  A. 
⎟⎟⎟⎟⎟⎠⎞⎜⎜⎜⎜⎜⎝⎛=naaaA!21i.e. 
Proof of Theorem 5.4.6  (1    2) 
)(TTTTnnaaaaaaAA!"2121⎟⎟⎟⎟⎟⎠⎞⎜⎜⎜⎜⎜⎝⎛=⎟⎟⎟⎟⎟⎠⎞⎜⎜⎜⎜⎜⎝⎛=TTTTTTTTTnnnnnnaaaaaaaaaaaaaaaaaa!"""!!212221212111⎟⎟⎟⎟⎟⎠⎞⎜⎜⎜⎜⎜⎝⎛⋅⋅⋅⋅⋅⋅⋅⋅⋅=nnnnnnaaaaaaaaaaaaaaaaaa!"""!!212221212111⇔
Proof of Theorem 5.4.6  (1     2) 
       AAT = I 
⎟⎟⎟⎟⎟⎠⎞⎜⎜⎜⎜⎜⎝⎛=⎟⎟⎟⎟⎟⎠⎞⎜⎜⎜⎜⎜⎝⎛⋅⋅⋅⋅⋅⋅⋅⋅⋅⇔11100!"###""nnnnnnaaaaaaaaaaaaaaaaaa212221212111      ai · ai = 1  for all  i  and  ai · aj = 0  for  i     j 
        {a1, a2, …, an}  is an orthonormal basis for  Rn 
⇔⇔⇔≠By Remark 5.4.4, we have shown that  A  is 
orthogonal if and only if the rows of  A  form an 
orthonormal basis for  Rn. 
Exercise 5.4.6 
Complete the proof of Theorem 5.4.6. 
That is, the proof of “1 ⇔ 3” is similar except we 
compute ATA instead.  
 
Theorem 5.4.7 
Let  S  and  T  be two orthonormal bases for a vector 
space. 
Then the transition matrix  P  from  S  to  T  is 
orthogonal. 
 
i.e.  P T  is the transition matrix from  T  to  S. 
Proof of Theorem 5.4.7 
Let  S = {u1, u2, …, uk}  and  T = {v1, v2, …, vk}. 
⎪⎪⎩⎪⎪⎨⎧⋅++⋅+⋅=⋅++⋅+⋅=⋅++⋅+⋅=kkkkkkkkkkvvuvvuvvuuvvuvvuvvuuvvuvvuvvuu)()()()()()()()()(!"!!22112222112212211111The transition matrix from  S  to  T  is 
⎟⎟⎟⎟⎟⎠⎞⎜⎜⎜⎜⎜⎝⎛⋅⋅⋅⋅⋅⋅⋅⋅⋅=kkkkkkvuvuvuvuvuvuvuvuvuP!"""!!212222111211
Proof of Theorem 5.4.7 
⎪⎪⎩⎪⎪⎨⎧⋅++⋅+⋅=⋅++⋅+⋅=⋅++⋅+⋅=kkkkkkkkkkuuvuuvuuvvuuvuuvuuvvuuvuuvuuvv)()()()()()()()()(!"!!22112222112212211111The transition matrix from  T  to  S  is 
   Q=v1⋅u1v2⋅u1!vk⋅u1v1⋅u2v2⋅u2!vk⋅u2"""v1⋅ukv2⋅uk!vk⋅uk⎛⎝⎜⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟⎟
Proof of Theorem 5.4.7 
⎟⎟⎟⎟⎟⎠⎞⎜⎜⎜⎜⎜⎝⎛⋅⋅⋅⋅⋅⋅⋅⋅⋅=kkkkkkvuvuvuvuvuvuvuvuvuP!"""!!212222111211Since  ui · vj = vj · ui  
for all  i, j,  we have  
Q = P T.  
 
By Theorem 3.7.5,  
Q = P –1. 
 
So  P –1 = P T,  i.e.  
P  is orthogonal. 
   Q=v1⋅u1v2⋅u1!vk⋅u1v1⋅u2v2⋅u2!vk⋅u2"""v1⋅ukv2⋅uk!vk⋅uk⎛⎝⎜⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟⎟
Example 5.4.8.1  (Rotation of  xy-coordinates) 
Let  S = {e1, e2}  be the standard basis for  R2, 
i.e.  e1 = (1, 0)  and  e2 = (0, 1). 
e1  is in the same direction as the  x-axis and   
e2  is in the same direction as the  y-axis. 
Consider a new  x’y’-coordinate system obtained by 
rotating the original  xy-coordinates counterclockwise 
about the origin through an angle   . θ
Example 5.4.8.1  (Rotation of  xy-coordinates) 
 y’                                                                   x’                                   
 y          e2                      e1           x  
Let  u1  and  u2  be unit vectors such that   
u1  is in the direction of the  x’-axis and   
u2  is in the direction of the  y’-axis. 
1u2uT = {u1, u2}  is an orthonormal basis for  R2. 
θθ
Example 5.4.8.1  (Rotation of  xy-coordinates) 
 y’                                                                   x’                                   
 y          e2                      e1           x  
1u2u⎩⎨⎧+−=−=+==212211eeueeu)cos()sin())cos(),sin(()sin()cos())sin(),(cos(θθθθθθθθThe transition matrix from  
T  to  S  is 
⎟⎠⎞⎜⎝⎛−=)cos()sin()sin()cos(θθθθPThe transition matrix from  
S  to  T  is 
⎟⎠⎞⎜⎝⎛−=)cos()sin()sin()cos(θθθθTPθθ
Example 5.4.8.1  (Rotation of  xy-coordinates) 
 
                  
Let  v = (x, y)  be a vector in  R2,  i.e.  (v)S = (x, y). 
 x               v                        y  
Let  (v)T = (x’, y’). 
 y’                               x’  θθ
Example 5.4.8.1  (Rotation of  xy-coordinates) 
⎟⎟⎠⎞⎜⎜⎝⎛===⎟⎠⎞⎜⎝⎛,y,xyxTSPvPv][][⎟⎠⎞⎜⎝⎛⎟⎠⎞⎜⎝⎛−=⎟⎠⎞⎜⎝⎛=⎟⎟⎠⎞⎜⎜⎝⎛⇒yxyx,y,xT)cos()sin()sin()cos(θθθθP⎩⎨⎧+−=+=⇒)cos()sin()sin()cos(θθθθyx,yyx,x
Example 5.4.8.2 
),,(),0,(),,(6162612121313131−=−==321uuu)0,,()0,,()1,0,0(21212121=−==321vvvS = {u1, u2 , u3}  and  T = {v1, v2 , v3}  are orthonormal 
bases for  R3. 
 
ui = (ui · v1)v1 + (ui · v2)v2 + (ui · v3)v3  for  i = 1, 2, 3. 
⎪⎪⎩⎪⎪⎨⎧−+=++−=+=32133212311vvvuvvvuvvu121123612121216231
Example 5.4.8.2 
The transition matrix from  S  to  T  is 
⎟⎟⎟⎟⎠⎞⎜⎜⎜⎜⎝⎛−−=1212162123216121310PThe transition matrix from  T  to  S  is 
   PT=13026−12121216312−112⎛⎝⎜⎜⎜⎜⎞⎠⎟⎟⎟⎟
Learning outcomes (Section 5.3) 
(5)  (Theorem 5.3.9) An alternative method to find a least 
squares solution to Ax = b.  
Learning outcomes (Section 5.4) 
(1)  The transition matrix from basis S to basis T was 
discussed in Chapter 3. What happens when both S and T 
are orthonormal bases?  
(3) While we have equivalent statements to ‘A is an 
invertible square matrix’, what can we say if A is an 
orthogonal matrix?  
(2) What is an orthogonal matrix and what properties does 
it have?  